{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUAzXFwp4szUMw/4Fc1aYO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/8johan/retinalseg/blob/main/Copie_de_retinal_seg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRMknGvzmEFR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import zipfile\n",
        "from U_net_models import unet ,attunet,SAunet\n",
        "from tools import imgDataset , create_dir,load_drivedata,load_chasdb1data,data_augment,augmentchasdb1_data,seeding,epoch_time,train,evaluate,calculate_metrics,masl_parse,Diceloss"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOwpZefapCLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9cEc1XGr3NE"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('/content/retinal.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/chasdb1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tLh-bjnpHoL"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('/content/Drive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86e3QbNMpSEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5htVhfO3TETl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "data_path = \"/content/drivedata/drivedata/DRIVE/\"\n",
        "(train_x, train_y), (test_x, test_y) = load_data(data_path)\n",
        "create_dir(\"new_drivedata/train/image/\")\n",
        "create_dir(\"new_drivedata/train/mask/\")\n",
        "create_dir(\"new_drivedata/test/image/\")\n",
        "create_dir(\"new_drivedata/test/mask/\")\n",
        "augment_data(train_x, train_y, \"new_drivedata/train/\", augment=True )\n",
        "augment_data(test_x, test_y, \"new_drivedata/test/\", augment=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (test_x, test_y) = load_data1(\"/content/chasdb1/retinal/\")\n",
        "\"\"\" Create directories to save the augmented data \"\"\"\n",
        "create_dir(\"new_chasdb1data/train/image/\")\n",
        "create_dir(\"new_chasdb1data/train/mask/\")\n",
        "create_dir(\"new_chasdb1data/test/image/\")\n",
        "create_dir(\"new_chasdb1data/test/mask/\")\n",
        "augment_datachasdb1(train_x, train_y, \"new_chasdb1data/train/\", augment=True)\n",
        "augment_datachasdb1(test_x, test_y, \"new_chasdb1data/test/\", augment=False)"
      ],
      "metadata": {
        "id": "RZkFApE4qNpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0oNRF4kqgEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykVRdm4lMlvB"
      },
      "outputs": [],
      "source": [
        "\n",
        "seeding(42)\n",
        "\"\"\" Directories \"\"\"\n",
        "create_dir(\"mymodel\")\n",
        "\n",
        "\"\"\" Load dataset \"\"\"\n",
        "train_x = sorted(glob(\"/content/new_drivedata/train/image/*\"))[:140]\n",
        "train_y = sorted(glob(\"/content/new_drivedata/train/mask/*\"))[:140]\n",
        "\n",
        "valid_x = sorted(glob(\"/content/new_drivedata/train/image/*\"))[150:200]\n",
        "valid_y = sorted(glob(\"/content/new_drivedata/train/mask/*\"))[150:200]\n",
        "\n",
        "data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n",
        "print(data_str)\n",
        "\n",
        "\"\"\" Hyperparameters \"\"\"\n",
        "H = 512\n",
        "W = 512\n",
        "size = (H, W)\n",
        "batch_size = 6\n",
        "num_epochs = 30\n",
        "lr = 0.001\n",
        "checkpoint_path = \"mymodel/checkpoint.pth\"\n",
        "\n",
        "\"\"\" Dataset and loader \"\"\"\n",
        "train_dataset = imgDataset(train_x, train_y)\n",
        "valid_dataset = imgDataset(valid_x, valid_y)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "device = torch.device('cuda')\n",
        "#model =build_unet()\n",
        "#model=attunet()\n",
        "#model =build_resunet()\n",
        "#model=attunet(3,1)\n",
        "model=unet(3,1)\n",
        "#model=SAunet(3,1)\n",
        "#model=unetPlusPlus()\n",
        "#model=resUnet()\n",
        "#model=attunet(3,1)\n",
        "#model=attUnet()\n",
        "#model=resUnet()\n",
        "#model=SA_UNet()\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
        "#loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#loss_fn=IoULoss()\n",
        "loss_fn=DiceLoss()\n",
        "trainingEpoch_loss=[]\n",
        "validationEpoch_loss=[]\n",
        "\"\"\" Training the model \"\"\"\n",
        "best_valid_loss = float(\"inf\")\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "  train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
        "  valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
        "  trainingEpoch_loss.append(train_loss)\n",
        "  if valid_loss < best_valid_loss:\n",
        "            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
        "            print(data_str)\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "  validationEpoch_loss.append(best_valid_loss)\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
        "  data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
        "  data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
        "  print(data_str)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mLUNKMyFrNVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuTlCORp1gfm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\" Seeding \"\"\"\n",
        "seeding(42)\n",
        "\n",
        "\"\"\" Folders \"\"\"\n",
        "create_dir(\"results\")\n",
        "\n",
        "\"\"\" Load dataset \"\"\"\n",
        "test_x = sorted(glob(\"/content/new_drivedata/test/image/*\"))\n",
        "test_y = sorted(glob(\"/content/new_drivedata/test/mask/*\"))\n",
        "\n",
        "\"\"\" Hyperparameters \"\"\"\n",
        "H = 512\n",
        "W = 512\n",
        "size = (W, H)\n",
        "checkpoint_path = \"mymodel/checkpoint.pth\"\n",
        "\n",
        "\"\"\" Load the checkpoint \"\"\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = unet(3,1)\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "time_taken = []\n",
        "\n",
        "for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n",
        "  name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "  image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "  x = np.transpose(image, (2, 0, 1))\n",
        "  x = x/255.0\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = x.astype(np.float32)\n",
        "  x = torch.from_numpy(x)\n",
        "  x = x.to(device)\n",
        "  mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "  y = np.expand_dims(mask, axis=0)            ## (1, 512, 512)\n",
        "  y = y/255.0\n",
        "  y = np.expand_dims(y, axis=0)               ## (1, 1, 512, 512)\n",
        "  y = y.astype(np.float32)\n",
        "  y = torch.from_numpy(y)\n",
        "  y = y.to(device)\n",
        "  with torch.no_grad():\n",
        "    jj\n",
        "\n",
        "    \"\"\" Prediction and Calculating FPS \"\"\"\n",
        "    start_time = time.time()\n",
        "    pred_y = model(x)\n",
        "    pred_y = torch.sigmoid(pred_y)\n",
        "    total_time = time.time() - start_time\n",
        "    time_taken.append(total_time)\n",
        "\n",
        "    score = calculate_metrics(y, pred_y)\n",
        "    metrics_score = list(map(add, metrics_score, score))\n",
        "    pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)\n",
        "    pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)\n",
        "    pred_y = pred_y > 0.5\n",
        "    pred_y = np.array(pred_y, dtype=np.uint8)\n",
        "\n",
        "\n",
        "  \"\"\" Saving masks \"\"\"\n",
        "  ori_mask = mask_parse(mask)\n",
        "  pred_y = mask_parse(pred_y)\n",
        "  line = np.ones((size[1], 10, 3)) * 128\n",
        "\n",
        "  cat_images = np.concatenate(\n",
        "            [image, line, ori_mask, line, pred_y * 255], axis=1\n",
        "        )\n",
        "  cv2.imwrite(f\"results/{name}\"\"unetdrive\"\".png\", cat_images)\n",
        "\n",
        "jaccard = metrics_score[0]/len(test_x)\n",
        "f1 = metrics_score[1]/len(test_x)\n",
        "recall = metrics_score[2]/len(test_x)\n",
        "precision = metrics_score[3]/len(test_x)\n",
        "acc = metrics_score[4]/len(test_x)\n",
        "print(f\"Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Recall: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f}\")\n",
        "\n",
        "fps = 1/np.mean(time_taken)\n",
        "print(\"FPS: \", fps)"
      ]
    }
  ]
}