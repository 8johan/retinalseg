{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5Lvc9bgTlRrhjqmKu/rsF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/8johan/retinalseg/blob/main/U_net_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K3rIYfcABf-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "class DropBlock(nn.Module):\n",
        "    def __init__(self, block_size: int = 5, p: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.p = p\n",
        "\n",
        "    def calculate_gamma(self, x: Tensor) -> float:\n",
        "        \"\"\"计算gamma\n",
        "        Args:\n",
        "            x (Tensor): 输入张量\n",
        "        Returns:\n",
        "            Tensor: gamma\n",
        "        \"\"\"\n",
        "\n",
        "        invalid = (1 - self.p) / (self.block_size ** 2)\n",
        "        valid = (x.shape[-1] ** 2) / ((x.shape[-1] - self.block_size + 1) ** 2)\n",
        "        return invalid * valid\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        N, C, H, W = x.size()\n",
        "        if self.training:\n",
        "            gamma = self.calculate_gamma(x)\n",
        "            mask_shape = (N, C, H - self.block_size + 1, W - self.block_size + 1)\n",
        "            mask = torch.bernoulli(torch.full(mask_shape, gamma, device=x.device))\n",
        "            mask = F.pad(mask, [self.block_size // 2] * 4, value=0)\n",
        "            mask_block = 1 - F.max_pool2d(\n",
        "                mask,\n",
        "                kernel_size=(self.block_size, self.block_size),\n",
        "                stride=(1, 1),\n",
        "                padding=(self.block_size // 2, self.block_size // 2),\n",
        "            )\n",
        "            x = mask_block * x * (mask_block.numel() / mask_block.sum())\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKS8M_DA9DK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQoX_mWR-VTY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "  def __init__(self,intc,outc):\n",
        "    super().__init__()\n",
        "    self.conv1=nn.Conv2d(intc,outc,kernel_size=3,padding=1,stride=1)\n",
        "    self.drop1= DropBlock(7, 0.9)\n",
        "    self.bn1=nn.BatchNorm2d(outc)\n",
        "    self.relu1=nn.ReLU()\n",
        "    self.conv2=nn.Conv2d(outc,outc,kernel_size=3,padding=1,stride=1)\n",
        "    self.drop2=DropBlock(7, 0.9)\n",
        "    self.bn2=nn.BatchNorm2d(outc)\n",
        "    self.relu2=nn.ReLU()\n",
        "    \n",
        "  def forward(self,input):\n",
        "    x=self.relu1(self.bn1(self.drop1(self.conv1(input))))\n",
        "    x=self.relu2(self.bn2(self.drop2(self.conv2(x))))\n",
        "    \n",
        "    return x\n",
        "class upconv(nn.Module):\n",
        "  def __init__(self,intc,outc) -> None:\n",
        "    super().__init__()\n",
        "    self.up=nn.ConvTranspose2d(intc, outc, kernel_size=2, stride=2, padding=0)\n",
        "   # self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,input):\n",
        "    x=self.up(input)\n",
        "    #x=self.relu(self.up(input))\n",
        "    return x \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMDWx9IUboqq"
      },
      "outputs": [],
      "source": [
        "class unet(nn.Module):\n",
        "  def __init__(self,int,out) -> None:\n",
        "    'int: represent the number of image channels'\n",
        "    'out: number of the desired final channels'\n",
        "\n",
        "    super().__init__()\n",
        "    'encoder'\n",
        "    self.convlayer1=double_conv(int,64)\n",
        "    self.down1=nn.MaxPool2d((2, 2))\n",
        "    self.convlayer2=double_conv(64,128)\n",
        "    self.down2=nn.MaxPool2d((2, 2))\n",
        "    self.convlayer3=double_conv(128,256)\n",
        "    self.down3=nn.MaxPool2d((2, 2))\n",
        "    self.convlayer4=double_conv(256,512)\n",
        "    self.down4=nn.MaxPool2d((2, 2))\n",
        "\n",
        "    'bridge'\n",
        "    self.bridge=double_conv(512,1024)\n",
        "    'decoder'\n",
        "    self.up1=upconv(1024,512)\n",
        "    self.convlayer5=double_conv(1024,512)\n",
        "    self.up2=upconv(512,256)\n",
        "    self.convlayer6=double_conv(512,256)\n",
        "    self.up3=upconv(256,128)\n",
        "    self.convlayer7=double_conv(256,128)\n",
        "    self.up4=upconv(128,64)\n",
        "    self.convlayer8=double_conv(128,64)\n",
        "    'output'\n",
        "    self.outputs = nn.Conv2d(64, out, kernel_size=1, padding=0)\n",
        "    self.sig=nn.Sigmoid()\n",
        "  def forward(self,input):\n",
        "    'encoder'\n",
        "    l1=self.convlayer1(input)\n",
        "    d1=self.down1(l1)\n",
        "    l2=self.convlayer2(d1)\n",
        "    d2=self.down2(l2)\n",
        "    l3=self.convlayer3(d2)\n",
        "    d3=self.down3(l3)\n",
        "    l4=self.convlayer4(d3)\n",
        "    d4=self.down4(l4)\n",
        "    'bridge'\n",
        "    bridge=self.bridge(d4)\n",
        "    'decoder'\n",
        "    up1=self.up1(bridge)\n",
        "    up1 = torch.cat([up1, l4], axis=1)\n",
        "    l5=self.convlayer5(up1)\n",
        "\n",
        "    up2=self.up2(l5)\n",
        "    up2 = torch.cat([up2, l3], axis=1)\n",
        "    l6=self.convlayer6(up2)\n",
        "\n",
        "    up3=self.up3(l6)\n",
        "    up3= torch.cat([up3, l2], axis=1)\n",
        "    l7=self.convlayer7(up3)\n",
        "\n",
        "    up4=self.up4(l7)\n",
        "    up4 = torch.cat([up4, l1], axis=1)\n",
        "    l8=self.convlayer8(up4)\n",
        "    out=self.outputs(l8)\n",
        "\n",
        "    #out=self.sig(self.outputs(l8))\n",
        "    return out \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gquUAQXB9MdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anydh9QYU-Ps"
      },
      "outputs": [],
      "source": [
        "class attentionGate(nn.Module):\n",
        "  def __init__(self,fg,fx,fint) :\n",
        "      super().__init__()\n",
        "      self.gatingSigconv=nn.Conv2d(fg, fg//2, kernel_size=3, padding=1)\n",
        "      self.bng = nn.BatchNorm2d(fint)\n",
        "      self.skipConnexion=nn.Conv2d(fx, fg//2, kernel_size=3,stride=2, padding=1)\n",
        "      self.bnskip = nn.BatchNorm2d(fint)\n",
        "      self.phi=nn.Conv2d(fg//2, 1, kernel_size=3, padding=1)\n",
        "      self.sig=nn.Sigmoid()\n",
        "      self.relu=nn.ReLU()\n",
        "      self.resampe=nn.Upsample(scale_factor=2)\n",
        "  def forward(self ,skip,gate):\n",
        "    \n",
        "    intm=self.relu(self.skipConnexion(skip)+self.gatingSigconv(gate))\n",
        "    \n",
        "    #make attentin coifeicent betzeen 0 and 1 \n",
        "    intm=self.sig(self.phi(intm))\n",
        "    #resample the attention matrix\n",
        "    intm=self.resampe(intm)\n",
        "    xi=skip*intm\n",
        "    return xi \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNHy1ll49YVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO3duIwRRj6E"
      },
      "outputs": [],
      "source": [
        "class attunet(nn.Module):\n",
        "  def __init__(self,intc,out) -> None:\n",
        "    'int: represent the number of image channels'\n",
        "    'out: number of the desired final channels'\n",
        "\n",
        "    super().__init__()\n",
        "    'encoder'\n",
        "    self.convlayer1=double_conv(intc,64)\n",
        "    self.down1=nn.MaxPool2d((2, 2))\n",
        "    self.convlayer2=double_conv(64,128)\n",
        "    self.down2=nn.MaxPool2d((2, 2))\n",
        "    self.convlayer3=double_conv(128,256)\n",
        "    self.down3=nn.MaxPool2d((2, 2))\n",
        "    self.convlayer4=double_conv(256,512)\n",
        "    self.down4=nn.MaxPool2d((2, 2))\n",
        "\n",
        "    'bridge'\n",
        "    self.bridge=double_conv(512,1024)\n",
        "    'decoder'\n",
        "    self.up1=upconv(1024,512)\n",
        "    self.convlayer5=double_conv(1024,512)\n",
        "    self.up2=upconv(512,256)\n",
        "    self.convlayer6=double_conv(512,256)\n",
        "    self.up3=upconv(256,128)\n",
        "    self.convlayer7=double_conv(256,128)\n",
        "    self.up4=upconv(128,64)\n",
        "    self.convlayer8=double_conv(128,64)\n",
        "    'output'\n",
        "    self.outputs = nn.Conv2d(64, out, kernel_size=1, padding=0)\n",
        "    self.sig=nn.Sigmoid()\n",
        "    'attention modules'\n",
        "    self.attgate1=attentionGate(1024,512,1024)\n",
        "    self.attgate2=attentionGate(512,256,512)\n",
        "    self.attgate3=attentionGate(256,128,256)\n",
        "    self.attgate4=attentionGate(128,64,128)\n",
        "  def forward(self,input):\n",
        "    'encoder'\n",
        "    l1=self.convlayer1(input)\n",
        "    d1=self.down1(l1)\n",
        "    l2=self.convlayer2(d1)\n",
        "    d2=self.down2(l2)\n",
        "    l3=self.convlayer3(d2)\n",
        "    d3=self.down3(l3)\n",
        "    l4=self.convlayer4(d3)\n",
        "    d4=self.down4(l4)\n",
        "    'bridge'\n",
        "    bridge=self.bridge(d4)\n",
        "    'decoder'\n",
        "    l4=self.attgate1(l4,bridge)\n",
        "    up1=self.up1(bridge)\n",
        "    up1 = torch.cat([up1, l4], axis=1)\n",
        "    l5=self.convlayer5(up1)\n",
        "\n",
        "    l3=self.attgate2(l3,l5)\n",
        "\n",
        "    up2=self.up2(l5)\n",
        "    up2 = torch.cat([up2, l3], axis=1)\n",
        "    l6=self.convlayer6(up2)\n",
        "\n",
        "\n",
        "    l2=self.attgate3(l2,l6)\n",
        "    up3=self.up3(l6)\n",
        "    up3= torch.cat([up3, l2], axis=1)\n",
        "    l7=self.convlayer7(up3)\n",
        "    l1=self.attgate4(l1,l7)\n",
        "\n",
        "    up4=self.up4(l7)\n",
        "    up4 = torch.cat([up4, l1], axis=1)\n",
        "    l8=self.convlayer8(up4)\n",
        "    out=self.outputs(l8)\n",
        "\n",
        "    #out=self.sig(self.outputs(l8))\n",
        "    return out "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwjSPpYp9eAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBs79tPhf4BR"
      },
      "outputs": [],
      "source": [
        "import torch.nn.init as init\n",
        "class reslblock(nn.Module):\n",
        "  def __init__(self,intc,outc) -> None:\n",
        "     super().__init__()\n",
        "     self.bn1=nn.BatchNorm2d(outc)\n",
        "     self.act1=nn.ReLU()\n",
        "     self.conv1=nn.Conv2d(intc,outc,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "     self.drop1=DropBlock(5,0.2)\n",
        "     self.bn2=nn.BatchNorm2d(outc)\n",
        "     self.act2=nn.ReLU()\n",
        "     self.act3=nn.ReLU()\n",
        "     self.conv2=nn.Conv2d(outc,outc,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "     self.drop2=DropBlock(5,0.7)\n",
        "     self.resskip=nn.Conv2d(intc,outc,kernel_size=1,stride=1,padding=0,bias=False)\n",
        "     self.bnsjip=nn.BatchNorm2d(outc)\n",
        "     #self.identity_downsample = identity_downsample\n",
        "  def forward(self,input):\n",
        "    \n",
        "    x=self.act1(self.bn1(self.drop1(self.conv1(input))))\n",
        "    x=self.act3(self.bn2(self.drop2(self.conv2(x))))\n",
        "    \n",
        "    skip=self.bnsjip(self.resskip(input))\n",
        "    \n",
        "    \n",
        "    x=x+skip\n",
        "    x=self.act2(x)\n",
        "    return x\n",
        "  \n",
        "    \n",
        "class resUnet(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "     super().__init__()\n",
        "     \n",
        "     self.resunit0=reslblock(3,64)\n",
        "     self.down1=nn.MaxPool2d((2,2))\n",
        "     self.resunit1=reslblock(64,128)\n",
        "     self.down2=nn.MaxPool2d((2,2))\n",
        "     self.resunit2=reslblock(128,256)\n",
        "     self.down3=nn.MaxPool2d((2,2))\n",
        "     self.resunit3=reslblock(256,512)\n",
        "     self.down4=nn.MaxPool2d((2,2))\n",
        "     'bridge'\n",
        "     self.resunit4=reslblock(512,1024)\n",
        "\n",
        "     'decoder'\n",
        "     self.up1=upconv(1024,512)\n",
        "     self.resunit5=reslblock(1024,512)\n",
        "     self.up2=upconv(512,256)\n",
        "     self.resunit6=reslblock(512,256)\n",
        "     self.up3=upconv(256,128)\n",
        "     self.resunit7=reslblock(256,128)\n",
        "     self.up4=upconv(128,64)\n",
        "     self.resunit8=reslblock(128,64)\n",
        "\n",
        "     self.output = nn.Conv2d(64, 1, kernel_size=1, padding=0,stride=1)\n",
        "     self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self,input):\n",
        "    \n",
        "    skip1=self.resunit0(input)\n",
        "    \n",
        "    x=self.down1(skip1)\n",
        "\n",
        "    'firts residual unit'\n",
        "    skip2=self.resunit1(x)\n",
        "    x=self.down2(skip2)\n",
        "    'second residual unit'\n",
        "    skip3=self.resunit2(x)\n",
        "    x=self.down3(skip3)\n",
        "    'therde residual unit'\n",
        "    skip4=self.resunit3(x)\n",
        "    x=self.down4(skip4)\n",
        "\n",
        "    'bridge '\n",
        "    x=self.resunit4(x)\n",
        "    'decoder 1'\n",
        "    x=self.up1(x)\n",
        "    x = torch.cat([x, skip4], axis=1)\n",
        "    x=self.resunit5(x)\n",
        "\n",
        "    x=self.up2(x)\n",
        "    x= torch.cat([x, skip3], axis=1)\n",
        "    x=self.resunit6(x)\n",
        "\n",
        "    x=self.up3(x)\n",
        "    x = torch.cat([x, skip2], axis=1)\n",
        "    x=self.resunit7(x)\n",
        "\n",
        "    x=self.up4(x)\n",
        "    x= torch.cat([x, skip1], axis=1)\n",
        "    \n",
        "    x=self.resunit8(x)\n",
        "    \n",
        "\n",
        "    out=self.sigmoid(self.output(x))\n",
        "    return out \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-8faqf69uug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgYOuyiTOE7H"
      },
      "outputs": [],
      "source": [
        "\n",
        "class unetPlusPlus(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "     super().__init__()\n",
        "     'backbone 1'\n",
        "     \n",
        "     \n",
        "     self.convlaye00=double_conv(3,64)\n",
        "     self.down1=nn.MaxPool2d((2, 2))\n",
        "     self.convlayer10=double_conv(64,128)\n",
        "     self.down2=nn.MaxPool2d((2, 2))\n",
        "     self.convlayer20=double_conv(128,256)\n",
        "     self.down3=nn.MaxPool2d((2, 2))\n",
        "     self.convlayer30=double_conv(256,512)\n",
        "     self.down4=nn.MaxPool2d((2, 2))\n",
        "     self.convlayer40=double_conv(512,1024)\n",
        "     'up to backbone 2'\n",
        "\n",
        "     self.up10to01=upconv(128,64)\n",
        "     self.up20to11=upconv(256,128)\n",
        "     self.up30to21=upconv(512,256)\n",
        "     self.up40to31=upconv(1024,512)\n",
        "     'backbone 2'\n",
        "     \n",
        "\n",
        "\n",
        "     self.convlayer01=double_conv(128,64)\n",
        "     self.convlayer11=double_conv(256,128)\n",
        "     self.convlayer21=double_conv(512,256)\n",
        "     self.convlayer31=double_conv(1024,512)\n",
        "\n",
        "     \n",
        "     'up to backbone 3'\n",
        "     self.up11to02=upconv(128,64)\n",
        "     self.up21to12=upconv(256,128)\n",
        "     self.up31to22=upconv(512,256)\n",
        "\n",
        "     'backbone 3'\n",
        "     self.convlayer02=double_conv(64*3,64)\n",
        "     self.convlayer12=double_conv(128*3,128)\n",
        "     self.convlayer22=double_conv(256*3,256)\n",
        "\n",
        "     'up to backbone 4'\n",
        "     self.up22to13=upconv(256,128)\n",
        "     self.up12to03=upconv(128,64)\n",
        "     'backbone 4'\n",
        "     self.convlayer03=double_conv(64*4,64)\n",
        "     self.convlayer13=double_conv(128*4,128)\n",
        "\n",
        "     'up to backbone 5'\n",
        "     self.up13to04=upconv(128,64)\n",
        "\n",
        "     'backbone 5'\n",
        "     self.convlayer04=double_conv(64*5,64)\n",
        "\n",
        "     'outputs'\n",
        "     self.output=nn.Conv2d(64,1,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "  def forward(self,input):\n",
        "    'backbone 1'\n",
        "    skip00=self.convlaye00(input)\n",
        "    x=self.down1(skip00)\n",
        "\n",
        "    skip10=self.convlayer10(x)\n",
        "    x=self.down2(skip10)\n",
        "\n",
        "    skip20=self.convlayer20(x)\n",
        "    x=self.down2(skip20)\n",
        "\n",
        "    skip30=self.convlayer30(x)\n",
        "    #x=self.down2(skip30)\n",
        "\n",
        "    #skip40=self.convlayer40(x)\n",
        "    ' backbone 2'\n",
        "    x=self.up10to01(skip10)\n",
        "    x= torch.cat([x, skip00], axis=1)\n",
        "    skip01=self.convlayer01(x)\n",
        "\n",
        "    x=self.up20to11(skip20)\n",
        "    x= torch.cat([x, skip10], axis=1)\n",
        "    skip11=self.convlayer11(x)\n",
        "\n",
        "\n",
        "    x=self.up30to21(skip30)\n",
        "    x= torch.cat([x, skip20], axis=1)\n",
        "    skip21=self.convlayer21(x)\n",
        "\n",
        "    #x=self.up40to31(skip40)\n",
        "    #x= torch.cat([x, skip30], axis=1)\n",
        "    #skip31=self.convlayer31(x)\n",
        "\n",
        "    'backbone 3'\n",
        "    x=self.up11to02(skip11)\n",
        "    x= torch.cat([x,skip00,skip01 ], axis=1)\n",
        "    skip02=self.convlayer02(x)\n",
        "\n",
        "    x=self.up21to12(skip21)\n",
        "    x= torch.cat([x,skip10,skip11 ], axis=1)\n",
        "    skip12=self.convlayer12(x)\n",
        "\n",
        "    #x=self.up31to22(skip31)\n",
        "    #x= torch.cat([x,skip20,skip21 ], axis=1)\n",
        "    #skip22=self.convlayer22(x) \n",
        "\n",
        "    'bckbone 4'\n",
        "    x=self.up12to03(skip12)\n",
        "    x= torch.cat([x,skip00,skip01,skip02 ], axis=1)\n",
        "    x=self.convlayer03(x)\n",
        "\n",
        "    #x=self.up22to13(skip22)\n",
        "    #x= torch.cat([x,skip10,skip11 ,skip12], axis=1)\n",
        "    #skip13=self.convlayer13(x) \n",
        "\n",
        "    'backbone 5'\n",
        "    #x=self.up13to04(skip13)\n",
        "    #x= torch.cat([x,skip00,skip01 ,skip02,skip03 ], axis=1)\n",
        "    #x=self.convlayer04(x)\n",
        "\n",
        "    'out'\n",
        "    out=self.output(x)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MikV3FcK-DLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLBW2WX6cgKP"
      },
      "outputs": [],
      "source": [
        "class spatialAttention(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "    \n",
        "    self.conv77=nn.Conv2d(2,1,kernel_size=7,padding=3)\n",
        "    self.sig=nn.Sigmoid()\n",
        "  def forward(self,input):\n",
        "    x=torch.cat( (torch.max(input,1)[0].unsqueeze(1), torch.mean(input,1).unsqueeze(1)), dim=1 )\n",
        "    x=self.sig(self.conv77(x))\n",
        "    x=input*x\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zp81s5lL-MTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpHgNzF0dJX2"
      },
      "outputs": [],
      "source": [
        "class SAunet(nn.Module):\n",
        "  def __init__(self,int,out) -> None:\n",
        "    'int: represent the number of image channels'\n",
        "    'out: number of the desired final channels'\n",
        "\n",
        "    super().__init__()\n",
        "    'encoder'\n",
        "    self.convlayer1=double_conv(int,64)\n",
        "    self.down1=nn.MaxPool2d((2, 2))\n",
        "    self.convlayer2=double_conv(64,128)\n",
        "    self.down2=nn.MaxPool2d((2, 2))\n",
        "    self.convlayer3=double_conv(128,256)\n",
        "    self.down3=nn.MaxPool2d((2, 2))\n",
        "    self.convlayer4=double_conv(256,512)\n",
        "    self.down4=nn.MaxPool2d((2, 2))\n",
        "\n",
        "    'bridge'\n",
        "    self.attmodule=spatialAttention()\n",
        "    self.bridge1=nn.Conv2d(512,1024,kernel_size=3,stride=1,padding=1)\n",
        "    self.bn1=nn.BatchNorm2d(1024)\n",
        "    self.act1=nn.ReLU()\n",
        "    self.bridge2=nn.Conv2d(1024,1024,kernel_size=3,stride=1,padding=1)\n",
        "    self.bn2=nn.BatchNorm2d(1024)\n",
        "    self.act2=nn.ReLU()\n",
        "    'decoder'\n",
        "    self.up1=upconv(1024,512)\n",
        "    self.convlayer5=double_conv(1024,512)\n",
        "    self.up2=upconv(512,256)\n",
        "    self.convlayer6=double_conv(512,256)\n",
        "    self.up3=upconv(256,128)\n",
        "    self.convlayer7=double_conv(256,128)\n",
        "    self.up4=upconv(128,64)\n",
        "    self.convlayer8=double_conv(128,64)\n",
        "    'output'\n",
        "    self.outputs = nn.Conv2d(64, out, kernel_size=1, padding=0)\n",
        "    self.sig=nn.Sigmoid()\n",
        "  def forward(self,input):\n",
        "    'encoder'\n",
        "    l1=self.convlayer1(input)\n",
        "    d1=self.down1(l1)\n",
        "    l2=self.convlayer2(d1)\n",
        "    d2=self.down2(l2)\n",
        "    l3=self.convlayer3(d2)\n",
        "    d3=self.down3(l3)\n",
        "    l4=self.convlayer4(d3)\n",
        "    d4=self.down4(l4)\n",
        "    'bridge'\n",
        "    b1=self.act1(self.bn1(self.bridge1(d4)))\n",
        "    att=self.attmodule(b1)\n",
        "    b2=self.act2(self.bn2(self.bridge2(att)))\n",
        "    'decoder'\n",
        "    up1=self.up1(b2)\n",
        "    up1 = torch.cat([up1, l4], axis=1)\n",
        "    l5=self.convlayer5(up1)\n",
        "\n",
        "    up2=self.up2(l5)\n",
        "    up2 = torch.cat([up2, l3], axis=1)\n",
        "    l6=self.convlayer6(up2)\n",
        "\n",
        "    up3=self.up3(l6)\n",
        "    up3= torch.cat([up3, l2], axis=1)\n",
        "    l7=self.convlayer7(up3)\n",
        "\n",
        "    up4=self.up4(l7)\n",
        "    up4 = torch.cat([up4, l1], axis=1)\n",
        "    l8=self.convlayer8(up4)\n",
        "    out=self.outputs(l8)\n",
        "\n",
        "    #out=self.sig(self.outputs(l8))\n",
        "    return out \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}